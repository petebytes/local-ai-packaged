# Docker Compose Test Matrix for Parallel CUDA Version Testing
# Run multiple CUDA versions simultaneously for faster comparison
#
# Usage:
#   docker compose -f docker-compose.test-matrix.yml up -d
#   docker compose -f docker-compose.test-matrix.yml logs -f
#   docker compose -f docker-compose.test-matrix.yml down
#
# Benefits:
#   - Test 3-4 CUDA versions in parallel (60-70% time savings)
#   - Isolated ports and networks prevent conflicts
#   - Health checks for automated pass/fail
#   - Easy comparison of running services

services:
  # WhisperX with CUDA 12.8 (RTX 5090 minimum)
  whisperx-12.8:
    build:
      context: ../whisperx
      dockerfile: Dockerfile
      args:
        CUDA_VERSION: "12.8"
    container_name: whisperx-test-12.8
    restart: "no"  # One-time test run
    ports:
      - "8001:8000"  # Isolated port
    networks:
      - test-matrix
    volumes:
      - whisperx-cache:/root/.cache
      - ../shared:/app/shared
      - hf-cache:/data/.huggingface
      - torch-cache:/data/.torch
    environment:
      - COMPUTE_TYPE=float16
      - BATCH_SIZE=32
      - HF_TOKEN=${HF_TOKEN:-}
      - HF_HOME=/data/.huggingface
      - TRANSFORMERS_CACHE=/data/.huggingface/transformers
      - TORCH_HOME=/data/.torch
      - TEST_LABEL=CUDA-12.8
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "python3", "-c", "import torch; assert torch.cuda.is_available(); import requests; requests.get('http://localhost:8000/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # WhisperX with CUDA 12.9 (Newer stable)
  whisperx-12.9:
    build:
      context: ../whisperx
      dockerfile: Dockerfile
      args:
        CUDA_VERSION: "12.9"
    container_name: whisperx-test-12.9
    restart: "no"
    ports:
      - "8002:8000"
    networks:
      - test-matrix
    volumes:
      - whisperx-cache:/root/.cache
      - ../shared:/app/shared
      - hf-cache:/data/.huggingface
      - torch-cache:/data/.torch
    environment:
      - COMPUTE_TYPE=float16
      - BATCH_SIZE=32
      - HF_TOKEN=${HF_TOKEN:-}
      - HF_HOME=/data/.huggingface
      - TRANSFORMERS_CACHE=/data/.huggingface/transformers
      - TORCH_HOME=/data/.torch
      - TEST_LABEL=CUDA-12.9
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "python3", "-c", "import torch; assert torch.cuda.is_available(); import requests; requests.get('http://localhost:8000/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # WhisperX with CUDA 13.0 (Bleeding edge)
  whisperx-13.0:
    build:
      context: ../whisperx
      dockerfile: Dockerfile
      args:
        CUDA_VERSION: "13.0"
    container_name: whisperx-test-13.0
    restart: "no"
    ports:
      - "8003:8000"
    networks:
      - test-matrix
    volumes:
      - whisperx-cache:/root/.cache
      - ../shared:/app/shared
      - hf-cache:/data/.huggingface
      - torch-cache:/data/.torch
    environment:
      - COMPUTE_TYPE=float16
      - BATCH_SIZE=32
      - HF_TOKEN=${HF_TOKEN:-}
      - HF_HOME=/data/.huggingface
      - TRANSFORMERS_CACHE=/data/.huggingface/transformers
      - TORCH_HOME=/data/.torch
      - TEST_LABEL=CUDA-13.0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "python3", "-c", "import torch; assert torch.cuda.is_available(); import requests; requests.get('http://localhost:8000/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Test orchestrator - monitors all services and collects results
  test-orchestrator:
    image: python:3.11-slim
    container_name: test-orchestrator
    restart: "no"
    networks:
      - test-matrix
    volumes:
      - ./benchmark:/benchmark
      - /var/run/docker.sock:/var/run/docker.sock:ro
    environment:
      - SERVICES=whisperx-12.8:8001,whisperx-12.9:8002,whisperx-13.0:8003
      - RESULTS_PATH=/benchmark/parallel-test-results.json
    command: >
      /bin/bash -c "
        pip install requests docker pyyaml &&
        python3 /benchmark/parallel-test-monitor.py
      "
    depends_on:
      - whisperx-12.8
      - whisperx-12.9
      - whisperx-13.0

volumes:
  whisperx-cache:
    driver: local
  hf-cache:
    driver: local
  torch-cache:
    driver: local

networks:
  test-matrix:
    name: cuda-test-matrix
    driver: bridge
