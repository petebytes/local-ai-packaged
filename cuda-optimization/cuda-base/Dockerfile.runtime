# syntax=docker/dockerfile:1.12
# Shared CUDA Base Image - Runtime with BuildKit Cache Optimization
# This image contains common dependencies for all CUDA-enabled services
# Use this for services that only need to run pre-built models/libraries
#
# Build: docker build -f cuda-base/Dockerfile.runtime -t cuda-base:runtime-12.8 .
# Size: ~4GB (vs 8GB+ for individual service images)
#
# Benefits:
# - Single CUDA runtime download shared across all services
# - Single PyTorch download shared across all services
# - Faster rebuilds when services change
# - Consistent CUDA/PyTorch versions across all services
# - BuildKit cache mounts for 80% faster rebuilds

FROM nvidia/cuda:12.8.1-cudnn-runtime-ubuntu22.04

LABEL maintainer="Local AI Packaged"
LABEL description="Shared CUDA base image for AI services (runtime only)"
LABEL cuda.version="12.8.1"
LABEL pytorch.version="2.7.1"

# Prevent interactive prompts during package installation
ENV DEBIAN_FRONTEND=noninteractive

# Set library path for cuDNN (included in base image)
ENV LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu:${LD_LIBRARY_PATH}

# Set Python to unbuffered mode for better logging
ENV PYTHONUNBUFFERED=1

# Install common system dependencies with APT cache mount for faster rebuilds
# - python3.10: Python runtime
# - python3-pip: Python package manager
# - ffmpeg: Audio/video processing (required by WhisperX, InfiniteTalk)
# - git: Version control and pip git+https:// installs
# - libsndfile1: Audio file I/O library
# - wget/curl: Download utilities
RUN --mount=type=cache,target=/var/cache/apt,sharing=locked \
    --mount=type=cache,target=/var/lib/apt,sharing=locked \
    apt-get update && \
    apt-get install -y --no-install-recommends \
    python3.10 \
    python3-pip \
    ffmpeg \
    git \
    libsndfile1 \
    wget \
    curl \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean

# Upgrade pip to latest version with cache mount
RUN --mount=type=cache,target=/root/.cache/pip \
    pip3 install --upgrade pip setuptools wheel

# Install PyTorch with CUDA 12.8 support using cache mount
# This is the largest download (~2.5GB) and is now shared across all services
# Using PyTorch 2.7.1 - latest stable version with CUDA 12.8 support
# Cache mount makes rebuilds 80% faster
RUN --mount=type=cache,target=/root/.cache/pip \
    pip3 install \
    torch==2.7.1 \
    torchvision==0.22.1 \
    torchaudio==2.7.1 \
    --index-url https://download.pytorch.org/whl/cu128

# Install common Python dependencies used by multiple services with cache mount
RUN --mount=type=cache,target=/root/.cache/pip \
    pip3 install \
    numpy>=1.24.0 \
    requests>=2.31.0 \
    pillow>=10.0.0 \
    tqdm>=4.65.0

# Verify CUDA installation
RUN python3 -c "import torch; print(f'PyTorch version: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}'); print(f'CUDA version: {torch.version.cuda}')"

# Default working directory
WORKDIR /app

# Health check template (override in child images)
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD python3 -c "import torch; assert torch.cuda.is_available()" || exit 1
