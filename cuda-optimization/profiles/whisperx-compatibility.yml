# WhisperX Compatibility Profile
# Maximum compatibility with older hardware and software
# Best for: Legacy systems, troubleshooting, broad GPU support

name: "Maximum Compatibility"
service: "whisperx"

docker:
  build_args:
    CUDA_VERSION: "12.1"  # Legacy but widely compatible

environment:
  COMPUTE_TYPE: "float16"
  BATCH_SIZE: 8  # Small batch for older GPUs
  HF_TOKEN: "${HF_TOKEN}"

  # Conservative settings
  CUDA_LAUNCH_BLOCKING: "1"  # Synchronous for debugging

  # Cache paths
  HF_HOME: "/data/.huggingface"
  TRANSFORMERS_CACHE: "/data/.huggingface/transformers"
  TORCH_HOME: "/data/.torch"

notes: |
  Maximum compatibility profile for legacy hardware.
  - CUDA 12.1 works with older drivers
  - Small batch size for limited VRAM
  - Synchronous execution for easier debugging

  Expected performance:
  - ~5-8x realtime transcription speed
  - ~4-6GB VRAM usage
  - Works with older GPUs and drivers

recommended_for:
  - RTX 30-series and older
  - Troubleshooting CUDA issues
  - Development and testing
  - Older NVIDIA drivers

warning: |
  ⚠️ This profile does NOT work with RTX 5090 (Blackwell architecture).
  RTX 5090 requires CUDA 12.8 minimum with PyTorch 2.7.1+cu128.
  Use 'whisperx-stability-focused' or 'whisperx-speed-optimized' for RTX 5090.
